import threading
from abc import abstractmethod
from functools import wraps
from typing import Optional, Callable, Dict, List, Any

import pandas as pd
import numpy as np

import utils
from data_accessors.base_data_accessor import BaseDataAccessor
from schema.data_summary import DataSummary


class DataFrameAccessor(BaseDataAccessor):
    def __init__(self, df: pd.DataFrame, column_description: Optional[dict] = None):
        super().__init__()
        self._df = df
        self.column_description = column_description
        self._data_summary = None
        self._quality_summary = None  # ç¼“å­˜è´¨é‡æ£€æŸ¥ç»“æœ

    def get_data_summary(self):
        return self._data_summary

    def get_quality_summary(self) -> Dict[str, Any]:
        """
        è·å–æ•°æ®è´¨é‡æ‘˜è¦
        
        Returns:
            åŒ…å«è´¨é‡è¯„çº§ã€ç¼ºå¤±å€¼ã€é‡å¤è¡Œã€é—®é¢˜åˆ—ç­‰ä¿¡æ¯çš„å­—å…¸
        """
        if self._quality_summary is not None:
            return self._quality_summary
        
        df = self._df
        if df is None or len(df) == 0:
            return {
                "quality_level": "âšª æ— æ•°æ®",
                "total_rows": 0,
                "total_columns": 0,
                "issues": ["æ•°æ®ä¸ºç©º"]
            }
        
        total_rows = len(df)
        total_columns = len(df.columns)
        total_cells = df.size
        
        # ç¼ºå¤±å€¼åˆ†æ
        missing_counts = df.isnull().sum()
        missing_cells = missing_counts.sum()
        missing_rate = (missing_cells / total_cells * 100) if total_cells > 0 else 0
        
        # æ‰¾å‡ºç¼ºå¤±ç‡é«˜çš„åˆ—ï¼ˆ>5%ï¼‰
        problem_columns = []
        for col in df.columns:
            col_missing_rate = df[col].isnull().mean() * 100
            if col_missing_rate > 5:
                problem_columns.append({
                    "column": col,
                    "missing_rate": round(col_missing_rate, 2),
                    "missing_count": int(df[col].isnull().sum())
                })
        
        # é‡å¤è¡Œæ£€æµ‹
        duplicate_rows = df.duplicated().sum()
        duplicate_rate = (duplicate_rows / total_rows * 100) if total_rows > 0 else 0
        
        # æ•°æ®ç±»å‹åˆ†æ
        dtype_summary = {
            "numeric": len(df.select_dtypes(include=[np.number]).columns),
            "string": len(df.select_dtypes(include=['object']).columns),
            "datetime": len(df.select_dtypes(include=['datetime64']).columns),
            "other": len(df.columns) - len(df.select_dtypes(include=[np.number, 'object', 'datetime64']).columns)
        }
        
        # å¼‚å¸¸å€¼æ£€æµ‹ï¼ˆä»…æ•°å€¼åˆ—ï¼Œä½¿ç”¨ IQR æ–¹æ³•ï¼‰
        outlier_columns = []
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            col_data = df[col].dropna()
            if len(col_data) > 0:
                Q1 = col_data.quantile(0.25)
                Q3 = col_data.quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                outliers = ((col_data < lower_bound) | (col_data > upper_bound)).sum()
                outlier_rate = outliers / len(col_data) * 100
                if outlier_rate > 5:  # å¼‚å¸¸å€¼è¶…è¿‡5%æ‰æŠ¥å‘Š
                    outlier_columns.append({
                        "column": col,
                        "outlier_count": int(outliers),
                        "outlier_rate": round(outlier_rate, 2)
                    })
        
        # è®¡ç®—è´¨é‡è¯„åˆ†å’Œè¯„çº§
        quality_score = 100
        issues = []
        recommendations = []
        
        # ç¼ºå¤±å€¼æ‰£åˆ†
        if missing_rate > 20:
            quality_score -= 30
            issues.append(f"æ•°æ®ç¼ºå¤±ä¸¥é‡ï¼Œæ•´ä½“ç¼ºå¤±ç‡ {missing_rate:.1f}%")
            recommendations.append("å»ºè®®è¿›è¡Œç¼ºå¤±å€¼å¤„ç†ï¼ˆå¡«å……æˆ–åˆ é™¤ï¼‰")
        elif missing_rate > 5:
            quality_score -= 15
            issues.append(f"å­˜åœ¨ç¼ºå¤±å€¼ï¼Œæ•´ä½“ç¼ºå¤±ç‡ {missing_rate:.1f}%")
            recommendations.append("éƒ¨åˆ†åˆ—æœ‰ç¼ºå¤±å€¼ï¼Œåˆ†ææ—¶éœ€æ³¨æ„")
        elif missing_rate > 0:
            quality_score -= 5
        
        # é‡å¤è¡Œæ‰£åˆ†
        if duplicate_rate > 10:
            quality_score -= 20
            issues.append(f"é‡å¤æ•°æ®è¾ƒå¤šï¼Œ{duplicate_rows} è¡Œé‡å¤ ({duplicate_rate:.1f}%)")
            recommendations.append("å»ºè®®å»é™¤é‡å¤è¡Œ")
        elif duplicate_rate > 1:
            quality_score -= 10
            issues.append(f"å­˜åœ¨ {duplicate_rows} è¡Œé‡å¤æ•°æ®")
        
        # å¼‚å¸¸å€¼æ‰£åˆ†
        if len(outlier_columns) > 0:
            quality_score -= min(len(outlier_columns) * 5, 15)
            issues.append(f"{len(outlier_columns)} ä¸ªæ•°å€¼åˆ—å­˜åœ¨è¾ƒå¤šå¼‚å¸¸å€¼")
            recommendations.append("æ•°å€¼åˆ—å­˜åœ¨å¼‚å¸¸å€¼ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®å‡†ç¡®æ€§")
        
        # ç¡®å®šè´¨é‡è¯„çº§
        if quality_score >= 90:
            quality_level = "ğŸŸ¢ ä¼˜ç§€"
        elif quality_score >= 75:
            quality_level = "ğŸŸ¡ è‰¯å¥½"
        elif quality_score >= 60:
            quality_level = "ğŸŸ  ä¸€èˆ¬"
        else:
            quality_level = "ğŸ”´ éœ€å…³æ³¨"
        
        self._quality_summary = {
            "quality_level": quality_level,
            "quality_score": max(0, round(quality_score, 1)),
            "total_rows": total_rows,
            "total_columns": total_columns,
            "total_cells": total_cells,
            "missing": {
                "total_missing": int(missing_cells),
                "missing_rate": round(missing_rate, 2),
                "problem_columns": problem_columns
            },
            "duplicates": {
                "duplicate_rows": int(duplicate_rows),
                "duplicate_rate": round(duplicate_rate, 2)
            },
            "dtype_summary": dtype_summary,
            "outliers": {
                "detection_method": "IQR",
                "detection_rule": "å€¼ < Q1-1.5Ã—IQR æˆ– å€¼ > Q3+1.5Ã—IQR",
                "outlier_columns": outlier_columns
            },
            "issues": issues,
            "recommendations": recommendations
        }
        
        return self._quality_summary

    def get_quality_description(self) -> str:
        """
        ç”Ÿæˆäººç±»å¯è¯»çš„è´¨é‡æè¿°ï¼ˆMarkdownæ ¼å¼ï¼‰
        """
        quality = self.get_quality_summary()
        
        desc = f"""
## ğŸ“Š æ•°æ®è´¨é‡æ¦‚å†µ
- **è´¨é‡è¯„çº§**: {quality['quality_level']} (è¯„åˆ†: {quality['quality_score']}/100)
- **æ•°æ®è§„æ¨¡**: {quality['total_rows']:,} è¡Œ Ã— {quality['total_columns']} åˆ—
- **ç¼ºå¤±ç‡**: {quality['missing']['missing_rate']:.2f}% ({quality['missing']['total_missing']:,}/{quality['total_cells']:,})
- **é‡å¤è¡Œ**: {quality['duplicates']['duplicate_rows']:,} è¡Œ ({quality['duplicates']['duplicate_rate']:.2f}%)
"""
        
        # æ•°æ®ç±»å‹åˆ†å¸ƒ
        dtype_summary = quality['dtype_summary']
        desc += f"- **åˆ—ç±»å‹**: æ•°å€¼å‹ {dtype_summary['numeric']} åˆ—, æ–‡æœ¬å‹ {dtype_summary['string']} åˆ—"
        if dtype_summary['datetime'] > 0:
            desc += f", æ—¥æœŸå‹ {dtype_summary['datetime']} åˆ—"
        desc += "\n"
        
        # é—®é¢˜åˆ—
        if quality['missing']['problem_columns']:
            desc += "\n### âš ï¸ éœ€å…³æ³¨çš„åˆ—\n"
            for col_info in quality['missing']['problem_columns'][:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ª
                desc += f"- **{col_info['column']}**: ç¼ºå¤± {col_info['missing_count']} ä¸ªå€¼ ({col_info['missing_rate']}%)\n"
        
        # å¼‚å¸¸å€¼åˆ—
        if quality['outliers']['outlier_columns']:
            desc += "\n### ğŸ“ˆ å­˜åœ¨å¼‚å¸¸å€¼çš„åˆ—\n"
            desc += "> æ£€æµ‹æ–¹æ³•ï¼šIQRï¼ˆå››åˆ†ä½è·ï¼‰æ³•ï¼Œåˆ¤å®šæ ‡å‡†ï¼šå€¼ < Q1-1.5Ã—IQR æˆ– å€¼ > Q3+1.5Ã—IQR\n\n"
            for col_info in quality['outliers']['outlier_columns'][:3]:  # æœ€å¤šæ˜¾ç¤º3ä¸ª
                desc += f"- **{col_info['column']}**: {col_info['outlier_count']} ä¸ªå¼‚å¸¸å€¼ ({col_info['outlier_rate']}%)\n"
        
        # å»ºè®®
        if quality['recommendations']:
            desc += "\n### ğŸ’¡ å»ºè®®\n"
            for rec in quality['recommendations']:
                desc += f"- {rec}\n"
        
        return desc.strip()

    def detect_data(self) -> DataSummary:
        ds_df = self._df
        self.logger.info(f"start detect data, record count: {len(ds_df)}")

        columns = ds_df.columns.tolist()
        data_preview = ds_df[:5].to_dict(orient='records')
        for row in data_preview:
            for k, v in row.items():
                row[k] = utils.process_df_value(row[k])

        dtypes = {col: str(ds_df[col].dtype) for col in ds_df}
        dtypes = {col: 'string' if dtype == 'object' else dtype for col, dtype in dtypes.items()}
        # æŒ‰é¢‘ç‡ç»Ÿè®¡
        column_values = {col: [utils.process_df_value(v) for v in ds_df[col].value_counts(dropna=False).index.tolist()[:25]] for col in ds_df.columns}

        # table_describe = 'test table describe'
        table_describe = ''
        # column_describes = {col: f'test value {v}' for col in range(len(ds_df.columns))}
        column_describes = self.column_description if self.column_description else {}
        data_summary = DataSummary(
            columns=columns,
            dtypes=dtypes,
            column_values=column_values,
            table_description=table_describe,
            column_descriptions=column_describes,
            column_min_values={col: str(ds_df[col].dropna().min()) for col in ds_df.columns if dtypes[col] != 'string'},
            column_max_values={col: str(ds_df[col].dropna().max()) for col in ds_df.columns if dtypes[col] != 'string'}
        )
        return data_summary

    def execute(self, code, func_name='analyze'):
        """
        æ‰§è¡Œä»£ç 
        :param code: ä»£ç 
        :param func_name: ä»£ç ä¸­çš„å…¥å£å‡½æ•°ï¼Œä¸»è¦ç”¨äºè·å–ä»£ç æ‰§è¡Œç»“æœï¼Œä¸promptä¸­å®šä¹‰çš„è®©LLMå®Œæˆçš„ä»£ç ç­¾åä¸€è‡´
        :return: ä»£ç æ‰§è¡Œç»“æœï¼Œpd.DataFrameç±»å‹
        """
        # åœ¨namespaceä¸­æ‰§è¡Œï¼Œä¸æŒ‡å®šçš„è¯ï¼Œå¸¦importè¯­å¥çš„ä»£ç ï¼Œåªåœ¨execçš„å±€éƒ¨ä½œç”¨åŸŸä¸­ï¼Œå‡½æ•°è°ƒç”¨æ—¶ï¼Œæ— æ³•ä½¿ç”¨è¿™äº›ä¾èµ–
        namespace = {'pd': pd}
        # namespace['dfs'] = [self._df.copy()]
        exec(code, namespace, namespace)
        df = self._df
        res = namespace[func_name](df)
        # res = namespace[func_name]([df.copy()])

        if isinstance(res, pd.DataFrame):
            ret_df = res
        elif isinstance(res, pd.Series):
            ret_df = utils.convert_series_to_dataframe(res)
        elif isinstance(res, dict):
            if res['type'] == 'dataframe':
                ret_df = res['value']
            else:
                ret_df = pd.DataFrame({'ç»“æœ': [res['value']]})
        else:
            ret_df = res
        return ret_df


    def get_type(self):
        return 'python'

    @property
    def dataframe(self):
        """
        å¯¹äºæ–‡ä»¶ç±»å‹çš„æ•°æ®ï¼Œé€šè¿‡æ­¤å±æ€§å¯ä»¥è·å–å…¨éƒ¨æ•°æ®ï¼Œæ•°æ®åº“ç±»å‹çš„å­ç±»æ— éœ€å®ç°
        :return:
        """
        return self._df

    @abstractmethod
    def load_data(self, filepath, **kwargs):
        pass

    @classmethod
    def cached_data_loader(cls, loader_func: Callable) -> Callable:
        cached = {}
        lock = threading.Lock()

        @wraps(loader_func)
        def wrapper(self, filepath, *args, **kwargs):
            cache_key = (filepath, self.__class__.__name__) + tuple(args) + tuple([f"{k}={v}" for k, v in kwargs.items()])
            # æ£€æŸ¥ç¼“å­˜ï¼ˆç¬¬ä¸€æ¬¡æ— é”æ£€æŸ¥ï¼‰
            import os
            # è·å–æ–‡ä»¶å½“å‰çš„ä¿®æ”¹æ—¶é—´
            current_mtime = None
            if os.path.exists(filepath):
                current_mtime = os.path.getmtime(filepath)
            
            if cache_key in cached:
                cached_mtime, cached_df = cached[cache_key]
                # åªæœ‰æ–‡ä»¶ä¿®æ”¹æ—¶é—´ä¸€è‡´æ—¶æ‰ä½¿ç”¨ç¼“å­˜
                if current_mtime is not None and cached_mtime == current_mtime:
                    self.logger.info(f'{cache_key} cache hit (mtime unchanged)')
                    return cached_df.copy()
                else:
                    self.logger.info(f'{cache_key} cache invalidated (file modified: {cached_mtime} -> {current_mtime})')


            with lock:
                # åŒé‡æ£€æŸ¥é¿å…ç«äº‰æ¡ä»¶
                if cache_key in cached:
                    cached_mtime, cached_df = cached[cache_key]
                    if current_mtime is not None and cached_mtime == current_mtime:
                        self.logger.info(f'{cache_key} cache hit in lock')
                        return cached_df.copy()

                self.logger.info(f'{cache_key} cache miss, loading file...')
                df = loader_func(self, filepath, *args, **kwargs)
                
                # å­˜å‚¨ä¿®æ”¹æ—¶é—´å’Œæ•°æ®
                if current_mtime is not None:
                    cached[cache_key] = (current_mtime, df)
                
                return df.copy()

        return wrapper
